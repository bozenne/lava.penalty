% Created 2017-03-02 to 17:52
% Intended LaTeX compiler: pdflatex
\documentclass{article}
               \usepackage{listings}
\usepackage{color}
\usepackage{amsmath}
\usepackage{array}
\usepackage[T1]{fontenc}
\usepackage{natbib}
               \usepackage{ifthen,xifthen,xargs}
\usepackage{amsmath,amssymb,latexsym,amsfonts,dsfont}
\usepackage{empheq}
\usepackage[most]{tcolorbox}
\usepackage{geometry}
\newgeometry{top=2cm,bottom=2cm,right=2.5cm,left=2.5cm}
\newgeometry{top=2cm,bottom=2cm,right=2.5cm,left=2.5cm}
\newcommandx\boxResult[1]{\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]\Large#1\end{tcolorbox}}
\input{1_MathNotations}

\lstset{
keywordstyle=\color{blue},
commentstyle=\color{red},stringstyle=\color[rgb]{0,.5,0},
literate={~}{$\sim$}{1},
basicstyle=\ttfamily\small,
columns=fullflexible,
breaklines=true,
breakatwhitespace=false,
numbers=left,
numberstyle=\ttfamily\tiny\color{gray},
stepnumber=1,
numbersep=10pt,
backgroundcolor=\color{white},
tabsize=4,
keepspaces=true,
showspaces=false,
showstringspaces=false,
xleftmargin=.23in,
frame=single,
basewidth={0.5em,0.4em},
}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\author{Brice Ozenne}
\date{\today}
\title{Derivation of the proximal operator relative to the lasso, ridge, group lasso and nuclear norm penalty}
\hypersetup{
 pdfauthor={Brice Ozenne},
 pdftitle={Derivation of the proximal operator relative to the lasso, ridge, group lasso and nuclear norm penalty},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 24.5.1 (Org mode 9.0.4)}, 
 pdflang={English}}
\begin{document}

\maketitle

\tableofcontents

\clearpage

\section{Subgradient}
\label{sec:org3cb9764}
For non-differentiable convex functions, $ z $ sub-gradient of $ f $ at $ x $ :
\[ f(x') \geq f(x) + <z,x'-x>  \forall x'' \in \Real^p\]
	
subdifferential of $ f $ at $ x $: $ \partial f (x) = \{z\} $


\subsection{L1 norm}
\label{sec:org615ea4a}

When $ x $ is not $ 0 $ then $ \partial |x| = sign(x) = \pm 1 $. For $ |h|<1 $ and all $ x \in \Real$ we have:
\[ f(x) = |x| \geq h x = f(0) + h (x-0) \]
By definition of the subgradient $ \partial f(0) = [-1;1] $

\subsection{Euclidean norm}
\label{sec:org7754336}
When $ x $ is not $ 0 $ then: $ \partial ||x||_2 = \partial  \sqrt{\sum_i x_i^2} =  \frac{2 (x_1,\ldots,x_n)}{2 \sqrt{\sum_i x_i^2}} = \frac{x}{||x||_2}$.

For $ ||h||_2<1 $ and all $ x \in \Real$ we have:
\[ f(x) = ||x||_2 \geq ||h||_2 ||x||_2 \geq \trans{h} x = f(0) + \trans{h} (x-0) \]
By definition of the subgradient $ \partial f(0) = \{h| ||h||_2\leq1\} $

\subsection{Nuclear norm}
\label{sec:orgc2478e1}

Denoting the SVD decomposition of $ x $:
\[ x = U\Sigma\trans{V} \]
\begin{eqnarray}
 ||x||_* &=& tr(\sqrt{\trans{x}x}) = tr(\sqrt{\trans{(U\Sigma\trans{V})}(U\Sigma\trans{V})})  \notag \\
 &=&tr(\sqrt{V\Sigma\trans{U}U\Sigma\trans{V}}) \notag \\
 &=&tr(\sqrt{V\Sigma^2\trans{V}}) \text{ (circularity of the trace)} \notag \\  
 &=&tr(\sqrt{\trans{V}V\Sigma^2}) \notag \\  
 &=&tr(\sqrt{\Sigma^2}) \notag \\  
 &=&tr(|\Sigma|_1) \notag
\end{eqnarray}
We are interested in the derivative of the functionnal \(F(x) = tr(|\Sigma(x)|_1) \). 
According to \cite{Watson1992} (theorem 2) the subdifferential of this functional is:
\begin{eqnarray*}
\partial F(x) &=& U diag(\partial(tr(|\Sigma|_1))) \trans{V} \\
&=& U diag(\sum_j \partial(|\sigma_j|_1)) \trans{V} \\
\end{eqnarray*}

\clearpage

\section{Proximal operator}
\label{sec:orgfbb51e0}
$
\begin{aligned}[t]
 prox_{\tau g} :& \Real^p \rightarrow \Real^p \\
                & \theta \mapsto \argMin[x][g(x)+\frac{1}{2 \tau} ||x-\theta||_2^2]  
\end{aligned}
$
\subsection{One penalty}
\label{sec:orgc1a56ca}
\subsubsection{Lasso}
\label{sec:org1b9b5bf}

The lasso penalty is $\mathcal{P}_1(\theta)=\lambda_1||\theta||_1$. \\
The subdifferential of the L1 norm at \(x\) is \(\partial ||x||_1 = s_1(x) \) with:
\[ s_1(x) = \left\{ \begin{array}{l} -1 \text{ if } x < 0 \\
                            {[}-1;1{]} \text{ if } x = 0 \\
                            1  \text{ if } x > 0 
                \end{array} \right. \]


The proximal operator can be computed solving:
\begin{eqnarray*}
prox_{\tau \mathcal{P}_1}(\theta) &=& \argMin[x][\lambda_1||x||_1+\frac{1}{2\tau} ||x-\theta||_2^2] \\
\partial_x\left(\lambda_1||x||_1+\frac{1}{2 \tau} ||x-\theta||_2^2 \right) 
       &=& 0 \\
       &=& \lambda_1 s_1(x)+\frac{1}{\tau} (x-\theta) \\
     x &=& \theta - \lambda_1 \tau s_1(x)	
\end{eqnarray*}
		
\begin{eqnarray}
x &=& \left\{ \begin{matrix}
\theta - \lambda_1 \tau &\text{ if } x>0  \\
0 & \text{ if } x=0  \\
\theta + \lambda_1 \tau &\text{ if } x<0  \\
\end{matrix}  \right. 
= \left\{ \begin{matrix}
\theta - \lambda_1 \tau &\text{ if } \theta\geq\lambda_1\tau  \\
0 & \text{ if } |\theta|\leq\lambda_1\tau  \\
\theta + \lambda_1 \tau &\text{ if } \theta\leq-\lambda_1\tau  \\
\end{matrix}  \right. \notag 
\end{eqnarray}
		
\boxResult{\[prox_{\tau \mathcal{P}_1}(\theta) = sign(\theta)(|\theta|-\lambda_1\tau)^+ \]}

\subsubsection{Ridge}
\label{sec:org6141ee3}
The ridge penalty is $\mathcal{P}_2(\theta)=\frac{\lambda_2}{2}||\theta||^2_2$. \\

The proximal operator can be computed solving:
\begin{eqnarray*}
prox_{\tau \mathcal{P}_2}(\theta) &=& \argMin[x][\frac{\lambda_2}{2}||x||^2_2+\frac{1}{2\tau} ||x-\theta||_2^2] \\
\partial_x\left(\frac{\lambda_2}{2}||x||^2_2+\frac{1}{2 \tau} ||x-\theta||_2^2 \right) 
       &=& 0 \\
       &=& \lambda_2 x+\frac{1}{\tau} (x-\theta) \\
     x &=& \frac{1}{1+\tau\lambda_2}\theta
\end{eqnarray*}

\boxResult{
\[prox_{\tau \mathcal{P}_2}(\theta) = \frac{1}{1+\tau\lambda_2}\theta \]
}

\subsubsection{Group Lasso}
\label{sec:orge51319d}

The group lasso penalty is $\mathcal{P}_G(\theta)=\lambda_G||\theta||_2$ where $\theta$ is a vector. \\
The subdifferential of the L2 norm at \(x\) is \(\partial ||x||_G = s_G(x) \) with:
\[ s_G(x)_j = \left\{ \begin{array}{l} \frac{x_j}{||x||_2} \text{ if } ||x||_2 > 0 \\
                              h ; ||h||_2<1 \text{ if } ||x||_2 = 0 \\
                 \end{array} \right. \]

The proximal operator can be computed solving:
\begin{eqnarray*}
prox_{\tau \mathcal{P}_G}(\theta) &=& \argMin[x][\lambda_G||x||_2+\frac{1}{2\tau} ||x-\theta||_2^2] \\
\partial_{x_j}\left(\lambda_G||x||_2+\frac{1}{2 \tau} ||x-\theta||_2^2 \right) 
       &=& 0 \\
       &=& \lambda_G s_G(x)_j+\frac{1}{\tau} (x_j-\theta_j) \\
     x_j &=& \theta_j - \lambda_G \tau s_G(x)_j	
\end{eqnarray*}
		
\begin{eqnarray}
x_j &=& \left\{ \begin{matrix}
\theta_j - \lambda_G \tau  \frac{\theta_j}{||\theta||_2} &\text{ if } ||x||_2>0  \\
0 & \text{ if } ||x||_2=0  \\
\end{matrix}  \right. 
= \left\{ \begin{matrix}
\theta_j - \lambda_G \tau \frac{\theta_j}{||\theta||_2} &\text{ if } ||\theta||_2\geq\lambda_G\tau  \\
0 & \text{ if } ||\theta||_2\leq\lambda_G\tau  \\
\end{matrix}  \right. \notag 
\end{eqnarray}
		
\boxResult{
\[prox_{\tau \mathcal{P}_G}(\theta) =\theta\left(1-\frac{\lambda_G\tau}{||\theta||_2}\right)^+ \]
}

\subsubsection{Nuclear norm}
\label{sec:orga180bfc}
The nuclear penalty is $\mathcal{P}_G(\theta)=\lambda_N||\theta||_N = tr(\sqrt{\trans{\theta}\theta})$ where $\theta$ is a matrix. \\
The subdifferential of the nuclear norm in \(x\) is \(\partial ||x||_N = s_N(x) \) with
\[ s_N(x) = U_x diag(s_1(\sigma_x)) \trans{V_x}  \]
where \(x = U_x diag(\sigma_x) V_x\)


The proximal operator can be computed solving:
\begin{eqnarray*}
prox_{\tau \mathcal{P}_N}(\theta) &=& \argMin[x][\lambda_N||x||_N+\frac{1}{2\tau} ||x-\theta||_2^2] \\
\partial_x\left(\lambda_N||x||_N+\frac{1}{2 \tau} ||x-\theta||_2^2 \right) 
       &=& 0 \\
       &=& \lambda_N s_N(x) +\frac{1}{\tau} (x-\theta) \\
     x &=& \theta - \lambda_G \tau s_N(x) \\
    U_x \Sigma_x \trans{V_x} &=& U \Sigma_\theta \trans{V} - \lambda_G \tau U_x diag(s_1(\sigma_x)) \trans{V_x} \\
    U_x \left( \Sigma_x + \lambda_G \tau diag(s_1(\sigma_x)) \right) \trans{V_x} &=& U \Sigma_\theta \trans{V} \\
\end{eqnarray*} 
Therefore 
\[\Sigma_x + \lambda_G \tau diag(s_1(\sigma_x)) = \Sigma_\theta\]
with \(U_x = U\) and \(V_x = V\) is a valid solution. See appendix for more on the unicity of the solution.

So \begin{eqnarray}
\sigma_x &=& \left\{ \begin{matrix}
\sigma_\theta - \lambda_N \tau &\text{ if } \sigma_x>0  \\
0 & \text{ if } \sigma_x=0  \\
\sigma_\theta + \lambda_N \tau &\text{ if } \sigma_x<0  \\
\end{matrix}  \right. 
= \left\{ \begin{matrix}
\theta - \lambda_N \tau &\text{ if } \sigma_\theta\geq\lambda_N\tau  \\
0 & \text{ if } |\sigma_\theta|\leq\lambda_N\tau  \\
\theta + \lambda_N \tau &\text{ if } \sigma_\theta\leq-\lambda_N\tau  \\
\end{matrix}  \right. \notag 
\end{eqnarray}
		
\boxResult{
\[prox_{\tau \mathcal{P}_N}(\theta) = U diag(sign(\sigma_\theta)(|\sigma_\theta|-\lambda_N\tau)^+ \trans{V} \]
}

\subsection{Combinaison of penalties}
\label{sec:org65e550e}
\subsubsection{Elastic Net}
\label{sec:org02a150e}
The elastic net penalty is $\mathcal{P}_{12}(\theta)=\lambda_1||\theta||_1 + \lambda_2 ||\theta||^2_2$. \\

The proximal operator can be computed solving:
\begin{eqnarray*}
prox_{\tau \mathcal{P}_{12}}(\theta) &=& \argMin[x][\lambda_1||x||_1 + \lambda_2 ||x||^2_2+\frac{1}{2\tau} ||x-\theta||_2^2] \\
\partial_x\left(\lambda_1||x||_1 + \lambda_2 ||x||^2_2+\frac{1}{2 \tau} ||x-\theta||_2^2 \right) 
       &=& 0 \\
       &=& \lambda_1 s+\lambda_2 x+\frac{1}{\tau} (x-\theta) \\
     x &=& \frac{1}{1+\tau\lambda_2} (\theta - \lambda_1 \tau s)
\end{eqnarray*}
		
\begin{eqnarray}
x &=& \left\{ \begin{matrix}
\frac{1}{1+\tau\lambda_2}(\theta - \lambda_1 \tau) &\text{ if } x>0  \\
0 & \text{ if } x=0  \\
\frac{1}{1+\tau\lambda_2}(\theta + \lambda_1 \tau) &\text{ if } x<0  \\
\end{matrix}  \right. 
= \left\{ \begin{matrix}
\frac{1}{1+\tau\lambda_2}(\theta - \lambda_1 \tau) &\text{ if } \theta\geq\lambda_1\tau  \\
0 & \text{ if } |\theta|\leq\lambda_1\tau  \\
\frac{1}{1+\tau\lambda_2}(\theta + \lambda_1 \tau) &\text{ if } \theta\leq-\lambda_1\tau  \\
\end{matrix}  \right. \notag \\
prox_{\tau \mathcal{P}_1}(\theta) &=& \frac{1}{1+\tau\lambda_2}(sign(\theta)(|\theta|-\lambda_1\tau)) \\
\end{eqnarray}
		
\boxResult{
\[prox_{\tau \mathcal{P}_1}(\theta) = prox_{\tau \mathcal{P}_2}(prox_{\tau \mathcal{P}_1}(\theta)) \]
}
\subsubsection{Sparse Group lasso}
\label{sec:orgb73b055}
The sparse group lasso penalty is $\mathcal{P}_{G1}(\theta)=\lambda_1||\theta||_1 + \lambda_G||\theta||_2$ where $\theta$ is a vector. \\
		
The proximal operator can be computed solving:
\begin{eqnarray*}
prox_{\tau \mathcal{P}_{G1}}(\theta) &=& \argMin[x][\lambda_1||\theta||_1 + \lambda_G||\theta||_2+\frac{1}{2\tau} ||x-\theta||_2^2] \\
\partial_{x_j}\left(\lambda_1||\theta||_1 + \lambda_G||\theta||_2+\frac{1}{2 \tau} ||x-\theta||_2^2 \right) 
       &=& 0 \\
       &=& \lambda_1 s_1(x_j)+\lambda_G s_G(x)_j+\frac{1}{\tau} (x_j-\theta_j) \\
     x_j &=& \theta_j - \lambda_1 \tau s_1(x_j) - \lambda_G \tau s_G(x)_j \\		
\end{eqnarray*}

If  \(x\) is not null then:
\begin{eqnarray*}
     x &=& \theta - \lambda_1 \tau s_1(x) - \lambda_G \tau \frac{x}{||x||_2} \\		
     x \left(1 +  \frac{\lambda_G \tau}{||x||_2}\right) &=& \theta - \lambda_1 \tau s_1(x) \\		
     x  &=& \frac{||x||_2}{||x||_2 + \lambda_G \tau} \left(\theta - \lambda_1 \tau s_1(x) \right)\\		
\end{eqnarray*}

Taking the L2 norm on both sides:
\begin{eqnarray*}
    | ||x||_2 + \lambda_G \tau | &=&  ||x||_2 + \lambda_G \tau   \\
     &=& ||x - \lambda_1 \tau s_1(x)||_2 \\		
     ||x||_2 &=& ||x - \lambda_1 \tau s_1(x)||_2 - \lambda_G \tau \\		
\end{eqnarray*}
So \(||x||_2>0\) implies \(||x - \lambda_1 \tau s_1(x)||_2 \geq \lambda_G \tau\)

Then:		
\begin{eqnarray*}
     x  &=&\frac{||x - \lambda_1 \tau s_1(x)||_2 - \lambda_G \tau}{||x - \lambda_1 \tau s_1(x)||_2} \left( x - \lambda_1 \tau s_1(x) \right)\\		
       &=& \left(1-\frac{\lambda_G \tau}{||x - \lambda_1 \tau s_1(x)||_2}\right) \left( x - \lambda_1 \tau s_1(x) \right)\\		
\end{eqnarray*}

\boxResult{
\[
prox {\tau \mathcal{P}_G}(\theta) =prox_{\tau \mathcal{P}_G}(prox_{\tau \mathcal{P}_1}(\theta)) 
\]
}

\subsubsection{others ?}
\label{sec:orgbeacd7a}

If the penalty is separable:  \(\mathcal{P}_{gh}(\theta)=\lambda_g g(\theta_g) + \lambda_h h(\theta_h) \)
Then
\begin{eqnarray*}
prox_{\tau \mathcal{P}_{gh}}(\theta) &=& \argMin[x][\lambda_g g(\theta_g) + \lambda_h h(\theta_h)+\frac{1}{2\tau} ||x-(\theta_f,\theta_g)||_2^2] \\
&=& \argMin[x][\lambda_g g(\theta_g) + \lambda_h h(\theta_h)+\frac{1}{2\tau} ||x_g-\theta_g||_2^2 + \frac{1}{2\tau} ||x_h-\theta_h||_2^2] \\
\end{eqnarray*}
So we can apply independently the proximal operator relative to each penalty.

\clearpage

\section{Appendix}
\label{sec:org539702e}

\subsection{Unicity of the SVD}
\label{sec:org78ca640}

\(A = U_1 \Sigma_1 V_1 = U_2 \Sigma_2 V_2\) 

\bigskip

Since \(U_1\), \(U_2\) and \(M = \trans{U_2} U_1\) are unitary matrices:
\begin{eqnarray*}
A \trans{A} &=&   U_1 \Sigma_1 \trans{\Sigma_1} \trans{U_1} =  U_2 \Sigma_2 \trans{\Sigma_2}  \trans{U_2}  \\
U_1 \Sigma_1 \trans{\Sigma_1} &=&  U_2 \Sigma_2 \trans{\Sigma_2}  \trans{U_2} U_1  \\
\trans{U_2} U_1 \Sigma_1 \trans{\Sigma_1} &=&  \Sigma_2 \trans{\Sigma_2}  \trans{U_2} U_1  \\
M \Sigma_1^2  &=& \Sigma_2^2   M  \\
\end{eqnarray*}

Therefore \(det(\Sigma_1) = det(\Sigma_2)\) and \(tr(\Sigma_1) = tr(\Sigma_2)\). 
So if there are 2 or less eigenvalues, \(\Sigma_1\) and\(\Sigma_2\) are equal in absolute value. 
By recurrence if it is true for p eigenvalues we consider a matrix with p+1 eigen vectors.
We can use an indicator vector \(x\) to project on a subspace of size p:
\begin{eqnarray*}
\trans{x} M \Sigma_1^2 x &=& \trans{x} \Sigma_2^2  M x \\
M' \Sigma_1^{2'} &=& \Sigma_2^{2'} M' \\
\end{eqnarray*}
Using the hypothesis of the recurrence we find that \(\Sigma_1\) and \(\Sigma_2\) must coincide in any subspace in absolute value.
Therefore they must be equal in absolute value. Denoting \(\epsilon\) a diagonal matrix filled with \(-1\) and \(1\) such that:
\begin{eqnarray*}
\Sigma_1 &=& \Sigma_2 \epsilon  \\
U_1 \Sigma_1 V_1 &=& U_2 \Sigma_1 \epsilon V_2 = U_2 \Sigma_1 V'_2
\end{eqnarray*}
Because \( \trans{(\epsilon V_2)} \epsilon V_2 = \trans{V_2} V_2   \) and \(\epsilon\) and \(V_2\) commute since \(\epsilon \) is diagonal. 
Therefore we can find \(U_2\) and \(V_2\) such that \(\Sigma_1 = \Sigma_2\)

\bigskip

Moreover:
\begin{eqnarray*}
\trans{U_2} U_1 \Sigma_1 \trans{\Sigma_1} &=&  \Sigma_1 \trans{\Sigma_1}  \trans{U_2} U_1  \\
M\Sigma &=&  \Sigma M\\
\end{eqnarray*}

We know that the eigenvectors of \(\Sigma\) are the canonical basis (\(\{e_j; j = 1 , \ldots, p \}\)). 
But since \(M\) and \(\Sigma\) commute, \(\{M e_j; j = 1 , \ldots p \}\) are also eigenvectors.
Since \(\Sigma\) has at most \(p\) eigen vectors then \(\{M e_j; j = 1 , \ldots p \} \propto \{e_j; j = 1 , \ldots p \}\)
and thus \(\{e_j; j = 1 , \ldots p \}\) are the eigenvector of \(M\). Then \(M\) is diagonal so \(U_1 = U U_2\) with \(U\) diagonal and unitary.

\subsection{Commutation and eigen vectors}
\label{sec:org2b277c9}
Let consider \(A\) and \(B\) two matrices that commutes. 

\(x\) eigenvector of \(A\) with eigenvalue \(\lambda \Leftrightarrow B x \) eigenvector of \(A\):
\[A B x = B A x = \lambda B x \]
The reciproque is also true:
\begin{eqnarray*}
B A x &=& A B x = \lambda B x \\
B^{-1} B A x &=&  \lambda B B^{-1} x \\
A x &=&  \lambda x \\
\end{eqnarray*}

Therefore if \(A\) has distinct eigenvalues \(x\) and \(Bx\) must correspond to the same eigenvector
and are thus linearly related. \(x\) is then an eigen vector for \(B\).
In addition if both are invertible and have n linear independent eigenvectors they must be the same.


\bibliographystyle{apalike}
\bibliography{biblio}
\end{document}